---
layout: post
title: "WGAN"
date: 2022-11-12
author: Khan Osama, Jeonghyeon Kim
categories: GAN
tags: WGAN, GAN
use_math: true
---

# WGAN[작성중]

# Introduction

WGAN 이란?
적대적 신경 생성망 GAN의 비용함수를 Wasserstein distance로 설정하여 최적화를 진행하는 신경망이다.

기존의 GAN은 이미지가 복잡해 짐에 따라 학습의 난이도가 아주 올라가게 되고 이로 인해 학습이 매우 불안정 해지는 문제가 발생하게 되었다.
WGAN에서는 학습의 불안정성의 원인을 metric(거리)이라 보고 이것을 수학적으로 해결하고자 하였다.

## Kullback-Leibler Divergence

![Untitled](/assets/WGAN/KL.png)  
기존의 두 분포 사이의 거리를 측정하기 위해 주로 쓰이는 KLD는 위수식과 같다.
하지만 KLD와 같은 metric은 분모에 위치하는 $P_g(x) = 0$ 이고,
$P_r(x) \ne 0$ 인 곳이 발생하게 된다면 발산하게 되는 문제점이 생긴다. 본 논문에서는 저차원에서 주로 이러한 문제가 발생한다고 서술하고 있다.

Inline: $\int f(x) dx$

\begin{equation}
\int f(x) dx
\end{equation}

\begin{equation*}
\int g(x) dx
\end{equation*}

Block:

$$
\int f(x) dx
$$

Let's test some inline math $x$, $y$, $x_1$, $y_1$.

Now a inline math with special character: $x'$, $x^*$.

Test a display math:

$$
   |\psi_1\rangle = a|0\rangle + b|1\rangle
$$

Is it O.K.?

Test a display math with equation number:
\begin{equation}
|\psi_1\rangle = a|0\rangle + b|1\rangle
\end{equation}
Is it O.K.?

Test a display math with equation number:

$$
  \begin{align}
    |\psi_1\rangle &= a|0\rangle + b|1\rangle \\
    |\psi_2\rangle &= c|0\rangle + d|1\rangle
  \end{align}
$$

Is it O.K.?

And test a display math without equaltion number:

$$
  \begin{align*}
    |\psi_1\rangle &= a|0\rangle + b|1\rangle \\
    |\psi_2\rangle &= c|0\rangle + d|1\rangle
  \end{align*}
$$

Is it O.K.?

Test a display math with equation number:
\begin{align}
|\psi_1\rangle &= a|0\rangle + b|1\rangle \\
|\psi_2\rangle &= c|0\rangle + d|1\rangle
\end{align}
Is it O.K.?

And test a display math without equaltion number: double back slash added
\begin{align}
|\psi_1\rangle &= a|0\rangle + b|1\rangle \\\\
|\psi_2\rangle &= c|0\rangle + d|1\rangle
\end{align}
Is it O.K.?
