---
layout: post
title:  "VQVAE : Neural Discrete Representation Learning"
date:   2022-11-12
author: Doyoung Kim, MinHyung Lee
categories: Latent-Variable
tags: vae vector_quantization
use_math: true
published: true
---


# VQ-VAE 블로그 정리

## Introduction

VQ-VAE 이전의 VAE를 활용한 논문들은 모두 Continous하게 데이터를 표현하기 위해 Distribution을 추정합니다. VAE가 등장함으로서 우리는 빠른 Sampling과 데이터의 latent vector에 대해 더 주목할 수 있다는 장점이 있었습니다. 그러나 우리가 다루는 데이터들(Image, Audio, Video 등) 대부분은 사실 이미 Digital 상에서 표현된 데이터들입니다. 

![/assets/VQVAE_img/Untitled](/assets/VQVAE_img/Untitled.png)

이들은 모두 Continous하게 표현되지 않고 위 그림에서 보인는 것처럼 사실은 Discrete 한 Digital number로 구성된 Dataset을 가지고 Input으로 학습하며 Output 또한 그렇게 표현을 만듭니다. 그렇기에 해당 논문에서는 Latent vector 또한 Discrete하게 표현되어야 한다고 주장합니다. 물론 과거에 이산적인 변수를 가지고 학습을 하는 것을 다른 이들이 안 떠올리지는 않았을 것입니다. 하지만 이산적인 변수는 Backprop이 불가능하여 해당 논문에서는 이산적인 표현을 살리면서 미분이 가능하도록 하는 대체 방안도 함께 제시합니다.. 이는 구조 및 loss부분에서 자세히 설명 드리겠습니다.

따라서 저자는 VAE와 이산 표현을 결합한 새로운 생성모델인 VQ-VAE라는 새로운 생성모델을 제안합니다. 여기서 말하는 VQ란 Vector Quantization의 약자로 VAE 구조에 VQ기능이 들어간 모델로서, 기존 VAE 구조에 Vector Quantisation(VQ)를 사용하여 기존에 continous하게 latent vector를 구성하던 것을 discrete한 latent vector로 만들어 아래와 같은 구조로 변경합니다.

![/assets/VQVAE_img/Untitled](/assets/VQVAE_img/Untitled%201.png)
# Model Structure

![https://user-images.githubusercontent.com/60708119/200171694-c35e6a49-f222-41e0-9f9e-9cfe8bf060cd.png](https://user-images.githubusercontent.com/60708119/200171694-c35e6a49-f222-41e0-9f9e-9cfe8bf060cd.png)

- n : batch size
- h : image height
- w : image width
- c : number of channels on the input image
- d : number of channels in the hidden state

# Processing

![https://user-images.githubusercontent.com/60708119/200171775-52cf265a-8534-4f71-97e0-24db6cc4a9cf.png](https://user-images.githubusercontent.com/60708119/200171775-52cf265a-8534-4f71-97e0-24db6cc4a9cf.png)

1. **Reshape** : 마지막 차원을 제외한 모든 차원이 하나로 결합되어 각 차원의 n×h×w 벡터를 갖습니다.
2. **Calculating distances** : n×h×w 벡터 각각에 대해 임베딩 딕셔너리의 k 벡터 각각으로부터 거리를 계산하여 (n×h×w, k) 형태의 행렬을 얻습니다.
3. **Argmin** : n×h×w 벡터 각각에 대해 사전에서 가장 가까운 k 벡터의 인덱스를 찾습니다.
4. **Index from dictionary** : n×h×w 벡터 각각에 대해 사전에서 가장 가까운 벡터를 색인화합니다.
5. **Reshape** : convert back to shape (n, h, w ,d)
6. **Copying gradients** : If you followed up till now you'd realize that it's not possible to train this architecture through backpropagation as the gradient won't flow through argmin. Hence we try to approximate by copying the gradients from z_q back to z_e. In this way we're not actually minimizing the loss function but are still able to pass some information back for training.

# Vector Quantisation

The posterior categorical distribution q(z|x) probabilities are defined as one-hot as follows

![https://user-images.githubusercontent.com/60708119/200172372-f21bb15c-798b-4a9b-89cd-e20139ce6844.png](https://user-images.githubusercontent.com/60708119/200172372-f21bb15c-798b-4a9b-89cd-e20139ce6844.png)

- z_e(x)*ze*(*x*) is the output of the encoder network
- IN VAE, *log* *p*(*x*) can bound with ELBO, VQ-VAE proposal distribution *q*(*z*=*k*∣*x*) is deterministic
    
    log\ p(x)
    
    q(z=k|x)
    
- By defining a simple uniform prior over z we obtain a KL divergence constant and equal to log K

# Model Structure

![VQVAE1](https://user-images.githubusercontent.com/60708119/200171694-c35e6a49-f222-41e0-9f9e-9cfe8bf060cd.png)  
- n : batch size
- h : image height
- w : image width
- c : number of channels on the input image
- d : number of channels in the hidden state

# Processing
![VQVAE2](https://user-images.githubusercontent.com/60708119/200171775-52cf265a-8534-4f71-97e0-24db6cc4a9cf.png)  
1. **Reshape** : all dimensions except the last one are combined into one so that we habe n×h×w vectors each if dimensionality d
2. **Calculating distances** : for each of the n×h×w vectors we calculate distance from each of k vectors of the embedding dictionary to obtain a matrix of shape (n×h×w, k)
3. **Argmin** : for each of the n×h×w vectors we find the index of closest of the k vectors from dictionary
4. **Index from dictionary** : index the closest vector from the dictionary for each of n×h×w vectors
5. **Reshape** : convert back to shape (n, h, w ,d)
6. **Copying gradients** : If you followed up till now you'd realize that it's not possible to train this architecture through backpropagation as the gradient won't flow through argmin. Hence we try to approximate by copying the gradients from z_q back to z_e. In this way we're not actually minimizing the loss function but are still able to pass some information back for training.

# Vector Quantisation
The posterior categorical distribution q(z|x) probabilities are defined as one-hot as follows  
<img width="792" alt="VQVAE3" src="https://user-images.githubusercontent.com/60708119/200172372-f21bb15c-798b-4a9b-89cd-e20139ce6844.png">  
- $z_e(x)$ is the output of the encoder network
- IN VAE, $ log\ p(x) $ can bound with ELBO, VQ-VAE proposal distribution $q(z=k|x)$ is deterministic
- By defining a simple uniform prior over z we obtain a KL divergence constant and equal to log K

<img width="785" alt="VQVAE4" src="https://user-images.githubusercontent.com/60708119/200172375-26896a4f-a80e-446f-a17b-de7ed2c55688.png"> 

- The representation $z_e(x)$ is passed through the discretizaion bottle neck followed by mapping onto the nearest element of embedding $e$ as given in equation 1 and 2.

<img width="669" alt="VQVAE5" src="https://user-images.githubusercontent.com/60708119/200172632-d6109327-d959-4e75-90c4-91cdc40a769a.png">    

- 모델은 입력 x로부터 받아 인코더를 통해 출력 $z_e(x)$를 생성함
- Discrete latent variable z는 shared embedding space인 e로부터 가장 가까운 neighborhoodd와의 distance 계산에 의해 결정됨. 즉 가장 가까운 vector로 결정
- e인 embedding space를 codebook으로 부르며, $e\ \in\ R^{K×D}$ 이며, 여기에서 K는 discrete latent space의 size가 됨(즉, K-way categorical)
- D는 각각의 latent embedding vector인 $e_i$ 의 차원 수
- 종합적으로, K 개의 embedding vector가 존재하며, 이를 수식으로 나타내면 $e_i\ \in\ R^D,\ i\ \in\ 1, 2, ..., K$ 가 됨  

# Loss function  
> Total Loss  
  
$$ L = log\ p(x|z_q(x)) + ||sg[z_e(x)]-e||_2^2\ + \beta||z_e(x)-sg[e]||_2^2  $$  
- sg : stand for the stopgradient operator
- Decoder optimized the first loss term only
- Encoder optimizes the first and the last loss terms
- Embeddings are optimized by the middle loss term
 

##
- Since VQ-VAE assume a uniform prior for z, the KL term that usually appears in the ELBO is constant w.r.t. the encoder parameters and can thus be ignored for training  
> Reconstruction Loss 
 
 $$ L= log\ p(x|z_q(x)) $$
### Reconstruction Loss : which optimizes the decoder and encoder
$$ z_q(x) = e_k, \quad where \quad k=argmin_j||z_e(x)-e_j||_2 \qquad (2)  $$  
- There is no real gradient defined for equation 2
- Approximate the gradient similar to the staright-through estimator
- Just copy gradients from decoder input $z_q(x)$ to encoder output $z_e(x)$

> Codebook Loss

$$ L = ||sg[z_e(x)]-e||_2^2 $$

### Codebook Loss : due to the fact that gradients bypass the embedding, we use a dictionary learning algorithm which uses an L2 error to move the embedding vectors $e_i$ towards the encoder output
- This loss term is only used for updating the dictionary

> Commitment Loss
$$ L = \beta||z_e(x)-sg[e]||_2^2 $$
### Commitment Loss : since the volume of the embedding space is dimensionless, it can grow arbirtarily if the embeddings $e_i$ do not train as fast as the encoder parameters, and thus we add a commitment loss to make sure that the encoder commits to an embedding
- Author found the resulting algorithm to be quite rebust to $\beta$, as the results did not vary for values of $\beta$ ranging from 0.1 to 2.0
- VQ-VAE use $\beta$ = 0.25 in all experiments  
** In general this would depend in the scale of reconstruction loss  

# Log-likelihood of the complete model

$$ log\ p(x) = log\ \sum_k d=p(x|z_k)p(z_k) $$
- Because the decoder $p(x|z)$ for $z = z_q(x)$ from MAP-inference
- the decoder should not allocate any probability mass to $p(x|z)$ for $z \ne z_q(x)$ once it has fully converged  
-> $ log\ p(x) \approx log\ p(x|z_q(x))p(z_q(x))$  
-> $ log\ p(x) \ge log\ p(x|z_q(x))p(z_q(x))$  (Jensen's inequality)

# Posterior Collapse
## Why the posterior collapse occurs
- When the decoder can sufficiently generate only past data without latent z
- The hypothesized Gaussian prior has no information in fact
- Gap between ELBO and evidence, failure of true posterior approximation
- Because the encoder does not express mearning z at the beginning of training

<img width="330" alt="VQVAE6" src="https://user-images.githubusercontent.com/60708119/200174355-d6ecbe62-aee0-47d2-8b8a-50a763d39a4d.png">
  
The approximate posterior mimics the prior as it is, and the model means that learning proceeds while ignoring the latent variable.


# PixelCNN Prior
After training VQ-VAE, train PixelCNN over the discrete latents for images  
Discrete latents vector generated by pre-trained PixelCNN enters the VQ-VAE decoder input and generate images

## Experiments

저자는 VQ-VAE를 통해 데이터를 Vector Quantization하게 표현하여 학습을 시킴으로써 성능이 월등히 높아졌다는 표현을 하지 않습니다. 오히려 아래와 같이 “Vector quantization으로 Latent를 표현하면  VAE만큼 성능이 나오더라 “라고 합니다.

![/assets/VQVAE_img/Untitled](/assets/VQVAE_img/Untitled%202.png)

실제로 결과 또한 아래와 같이 기존 VAE와 좀  떨어지고 Monte Carlo 방법으로 생성한 VIMCO보다는 좋은 성능 결과를 제시한다.

![/assets/VQVAE_img/Untitled](/assets/VQVAE_img/Untitled%203.png)

하지만 이미지 자체를 눈으로 보게되면, 그래도 VAE보다는 그럴듯한 그림을 만들어낸다는 것을 알 수 있다. 아래 사진에서 배 모양 그림을 보면 Detail 한 영역에서는 알수 없는 그림을 내놓는 것도 확인 할 수 있다.

![/assets/VQVAE_img/Untitled](/assets/VQVAE_img/Untitled%204.png)

위 실험의 경우 차원을 128x128x3에서 z를 32x32x1 까지 42.6 배를 축소하였음에도 이미지의 중요한 정보를 잃지 않음을 알 수 있었다.

![/assets/VQVAE_img/Untitled](/assets/VQVAE_img/Untitled%205.png)

또한 음성에 대해서도 실험을 진행하였는데, VQ-VAE를 오직 long-term 상관관계(정보)만을 보존하도록 latent space를 만든 뒤 decoder로 복원하는 작업을 수행하였다.  예시를 들어보면 알 수 있겠지만, 그 내용은 변하지 않았으나 운율은 상당히 바뀐 것을 들을 수 있다. 이는 VQ-VAE가 어떤 언어적 지도 없이 고수준의 추상 공간을 이해하고 중요하지 않은 부분은 버리며 음성의 내용만을 잡아냈음을 뜻한다.

- 위에 음성이 Original, 아래 음성이 Reconstruction한 음성이다.

Test A:

[https://avdnoord.github.io/homepage/audio/pt1_orig1.wav](https://avdnoord.github.io/homepage/audio/pt1_orig1.wav)

[https://avdnoord.github.io/homepage/audio/pt1_recon1.wav](https://avdnoord.github.io/homepage/audio/pt1_recon1.wav)

Test B:

[https://avdnoord.github.io/homepage/audio/pt1_orig2.wav](https://avdnoord.github.io/homepage/audio/pt1_orig2.wav)

[https://avdnoord.github.io/homepage/audio/pt3_transfer2.wav](https://avdnoord.github.io/homepage/audio/pt3_transfer2.wav)

Test C:

[https://avdnoord.github.io/homepage/audio/pt3_source3.wav](https://avdnoord.github.io/homepage/audio/pt3_source3.wav)

[https://avdnoord.github.io/homepage/audio/pt3_transfer3.wav](https://avdnoord.github.io/homepage/audio/pt3_transfer3.wav)

Video에서도 실험을 진행하였는데, DeepMind Lab 환경에서 처음 6 frame이 주어지면 나머지 10 frame을 이어지는 내용으로 채우는 task로, 여기서 VQ-VAE가 하는 것은 오직 latent space(z_t) 상에서만 생성할 뿐 이미지를 직접 생성하지 않는다.

sequence xi안의 각 이미지는 prior model만 사용하여 모든 latent를 생성한 후 deterministic decoder로 대응되는 latent와 mapping시켜서 생성하게 된다. 따라서 VQ-VAE는 latent space 안에서만 수행하고 pixel space에서는 작업하지 않는다. 생성 결과는 아래 그림과 같다.

![/assets/VQVAE_img/Untitled](/assets/VQVAE_img/Untitled%206.png)

## Conclusion

- VAE와 discrete latent 표현을 위한 VQ를 결합하여 새로운 생성 모델을 만들었다.
- VQ-VAE는 long-term dependency를 잘 모델링할 수 있다.
- 원본 source를 작은 latent로 수십 배 압축할 수 있다.
- 이러한 latent는 discrete하며, continuous latent와 비교하여 성능이 필적할 만하다.
- Image, audio, video 모두에 대해서 잘 모델링 및 압축한 후 중요한 내용을 잘 보존하면서 복원이 가능하다.

## Future Work

![/assets/VQVAE_img/Untitled](/assets/VQVAE_img/Untitled%207.png)

Image Experiment때 보았지만, 음성에서는 탁월한 성능을 내지만 디테일한 영역에서는 아직 표현이 어렵다. 따라서 이 이미지에 특화된 VQ-VAE모델을 만든것이 VQ-VAE2이다. 

VQ-VAE2는 Large scale 이미지 생성 문제에 집중한 모델로,  prior를 더욱 강화하여 이전 보다 더 일관성, 현실성 있는 이미지 생성한다. 해당 이미지 결과는 그럴듯한 이미지를 만들기로 유명한 GAN과 비슷한성능을 낸다. 핵심적인 내용은 VQ-VAE와 다르게 더 큰 이미지를 다룰 수 있기 위해 vector quantized codes를 계층 구조로 사용한다는 것이다. 먼저, Bottom latent code에서 세부적인 부분을 모델링하고  bottom latent code를 한번 더 인코딩한 Top latent code는 global한 특징을 모델링함으로써 이미지 Feature을 더 잘 표현하게 만든다.
